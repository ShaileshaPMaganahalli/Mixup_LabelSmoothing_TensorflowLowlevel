{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientTape Basics and Linearclassifier from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOD/ZC9Ug5Zopp3rZzISfe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilalithaveerubhotla/Mixup_LabelSmoothing_TensorflowLowlevel/blob/master/GradientTape_Basics_and_Linearclassifier_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX1Vg6-K5Cxj",
        "colab_type": "text"
      },
      "source": [
        "## How Gradient tape works ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmFYGK_S4iHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CsaEcP05MrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wyM-dkT5toQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.random.normal(shape =(3,3))\n",
        "b = tf.random.normal(shape =(3,3))\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(a)\n",
        "  c = tf.sqrt(tf.sqrt(a)+tf.square(a)+b)\n",
        "  c_a = tape.gradient(c,a)\n",
        "  print(c_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rls5Oao07WbB",
        "colab_type": "text"
      },
      "source": [
        "# Linear regression using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVR0wrZG7CLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 2\n",
        "output_dim = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "# weight matrixx\n",
        "w = tf.Variable(tf.random.uniform(shape=(input_dim, output_dim)))\n",
        "\n",
        "# Bias\n",
        "b = tf.Variable(tf.zeros(shape=(output_dim,)))\n",
        "\n",
        "# Predictions function\n",
        "def prediction(features):\n",
        "  return tf.matmul(features,w)+b\n",
        "\n",
        "# loss Function\n",
        "def lossfn(labels, predictions):\n",
        "  return tf.reduce_mean(tf.square(labels - predictions))\n",
        "\n",
        "# Training function\n",
        "def training(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = prediction(x)\n",
        "      loss = lossfn(y, predictions)\n",
        "    # Note that `tape.gradient` works with a list as well (w, b).\n",
        "      dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
        "    w.assign_sub(learning_rate * dloss_dw)\n",
        "    b.assign_sub(learning_rate * dloss_db)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBQBLHAUSHPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# artificial data to test the linear model\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Prepare a dataset.\n",
        "num_samples = 10000\n",
        "negative_samples = np.random.multivariate_normal(\n",
        "    mean=[0, 10], cov=[[1, 0.75],[0.75, 1]], size=num_samples)\n",
        "positive_samples = np.random.multivariate_normal(\n",
        "    mean=[10, 0], cov=[[1, 0.75],[0.75, 1]], size=num_samples)\n",
        "features = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
        "labels = np.vstack((np.zeros((num_samples, 1), dtype='float32'),\n",
        "                    np.ones((num_samples, 1), dtype='float32')))\n",
        "\n",
        "plt.scatter(features[:, 0], features[:, 1], c=labels[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ahNzP5_Tv1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle the data.\n",
        "indices = np.random.permutation(len(features))\n",
        "features = features[indices]\n",
        "\n",
        "labels = labels[indices]\n",
        "\n",
        "# Create a tf.data.Dataset object for easy batched iteration\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(256)\n",
        "\n",
        "for epoch in range(10):\n",
        "  for step, (x, y) in enumerate(dataset):\n",
        "    loss = training(x, y)\n",
        "  print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF9JsUa0T4pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = prediction(features)\n",
        "plt.scatter(features[:, 0], features[:, 1], c=predictions[:, 0] > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdgaklP_VFkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch in range(20):\n",
        "  for step, (x, y) in enumerate(dataset):\n",
        "    loss = training(x, y)\n",
        "t_end = time.time() - t0\n",
        "print('Time per epoch: %.3f s' % (t_end / 20,))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAffUnBeVbIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = compute_predictions(x)\n",
        "    loss = compute_loss(y, predictions)\n",
        "    dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
        "  w.assign_sub(learning_rate * dloss_dw)\n",
        "  b.assign_sub(learning_rate * dloss_db)\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1HC57paVjD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t0 = time.time()\n",
        "for epoch in range(20):\n",
        "  for step, (x, y) in enumerate(dataset):\n",
        "    loss = training(x, y)\n",
        "t_end = time.time() - t0\n",
        "print('Time per epoch: %.3f s' % (t_end / 20,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6L7BrVMVk9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}